# 1.网络爬虫的分类

1. 通用网络爬虫:爬行范围和数量巨大
2. 聚焦网络爬虫:是按照预先定好的主题,有选择地进行相关网页爬取的一种爬虫
3. 增量式网络爬虫:在爬取网页时,只会在需要时爬行新产生或发生更新的页面,对于没有发生变化的页面则不会爬取
4. 深层网络爬虫:大部分内容被隐藏在搜索表单后面,不能通过静态链接来获取

# 2.网络爬虫的基本工作流程如下

1. 获取初始的URL地址,该地址是用户自己制定的初始爬取的网页
2. 爬取对应URL地址的网页,获取新的URL地址
3. 抽取新的URL地址放入URL队列中
4. 从URL队列中读取新的URL,然后依据新的URL下载网页,同时从新的网页中获取新的URL地址,重复上述的爬取过程
5. 设置停止条件，如果没有设置停止条件,爬虫会一直爬取下去,直到无法获取新的URL地址为止,设置停止地址后，爬虫将会在满足条件时停止爬取

# 3.python的网络请求

## 1.urllib模块的使用

子模块的使用

1. urllib.request：定义了打开URL的方法和类,如身份验证，重定向，cookie等
2. urllib.error:主要包含异常类,基本的异常类URLError
3. urllib.parse:URL的解析和URL的引用
4. urllib.robotparser:解析robots.txt

### 1.get请求方式发送请求并读取网页的内容

```python
import urllib.request #导入模块



response=urllib.request.urlopen('http://www.baidu.com') #打开指定需要爬取的网页
html=response.read() #读取网页源代码
print(html) #打印网页源代码
```

### 2.post请求获取网页信息

```python
import urllib.request #导入模块



data=bytes(urllib.parse.urlencode({'name':'liuyong'}),encoding='utf8') #将数据转换为字节流,并指定编码格式,name=liuyong为表单参数
response=urllib.request.urlopen('https://baidu.com',data=data) #打开网址,并传入数据  
html=response.read().decode('utf-8')  #读取网页内容,并指定编码格式  
print(html) #打印网页内容     
```

##    2.urllib3模块的使用

### 1.Get请求方式发送网络请求

```python
import urllib3

http = urllib3.PoolManager()  #创建一个连接池,用于处理与线程池的连接以及线程安全的所有的细节
response = http.request('GET', 'https://baidu.com')  #使用连接池来请求网页，对需要爬取的网页发送请求
print(response.data)  #打印网页的源代码     

```

### 2.Post请求方式发送网络请求

```python
import urllib3



http = urllib3.PoolManager()  #创建一个连接池,用于处理与线程池的连接以及线程安全的所有的细节
response=http.request('post', #制定请求方式位Post
'http://www.baidu.com',     #使用连接池来请求网页,发送请求
 fields={'name':'liuyong','studentid':'1906014118'})  #发送请求的参数,这里是一个字典
print(response.data)  #打印返回的数据
```

## 3.requests模块的使用

requests模块是pthon中实现HTTP请求的一种方式

### 1.Get请求方式打印多种请求信息

```python
import requests


response=requests.get("http://www.baidu.com")  # 发送请求
print(response.status_code)  # 打印状态码   
print(response.url)      # 打印请求的url
print(response.headers) #打印请求头
print(response.cookies)   #打印cookie
print(response.text)   #以文本形式打印网页源码
print(response.content) #以字节流形式打印网页源码
```

### 2.Post请求方式发送HTTP网络请求

```python
import requests


data={'name':'liuyong','age':18,'stduentid':'1906014118'}   #定义请求的表单参数
response=requests.post('http://baike.baidu.com/api/openapi/BaikeLemmaCardApi',data=data)  #发送post请求,并且携带表单参数data
print(response.text)  #打印响应的内容
```

requests模块除以上的两种请求方式外,还提供put,delete,head,options这几网络爬虫



注意:如果请求参数是在?之后的例如httpbin.org/get?key=val,允许用户使用param关键字参数以字典的形式传递参数

```python
import requests


data={'name':'liuyong','age':18,'stduentid':'1906014118'}   #定义请求的表单参数
response=requests.post('http://baike.baidu.com/api/openapi/BaikeLemmaCardApi',params=data)  #发送post请求,并且携带表单参数data,params是请求参数在?后面的
print(response.text)  #打印响应的内容

```

# 4.请求头的处理

在请求一个网页内容时,发现无论使用GET还是POST请求方式,都会出现403报错,这是因为服务器的反爬机制拒绝你的访问,可以模拟浏览器的头部信息来进行访问,这样就可以解决反爬机制



1. 查看头部信息,在想爬取的网站上f12打开开发者模式,发送一条访问,在开发者模式中打开之前发起的访问,找到请求头复制

   user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Mobile Safari/537.36 Edg/105.0.1343.33

2. 携带请求头的头部信息发送请求

```python
import requests


url='https://ww.baidu.com'   #设置需要爬取的网页链接
#创建头部信息
headers={'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Mobile Safari/537.36 Edg/105.0.1343.33'}
response=requests.get(url,headers=headers)  #爬取网页  
response.encoding='utf-8'  #设置编码格式
print(response.text)  #打印网页内容
```

# 5.网络超时

在访问一个网页时，如果该网页长时间未响应系统会判断网络超时,所以无法打开网页,下列示例模拟一个网络超时的案例

```python
import requests



#循环发送请求50次
for i in range(500):
  try:   #捕获异常
     response=requests.get('http://www.baidu.com',timeout=0.1)  #发送请求并且设置超时时间为0.5秒
     print(response.status_code)  #打印状态码
  except Exception as e:  #捕获异常
     print(str(e))  #打印异常信息
```

在上述程序的运行中,会发现有超时的现象出现

除了超时外,requests模块中提供3种常见的网络异常类

```python
from shutil import ReadError
from urllib.error import HTTPError
from urllib.robotparser import RequestRate
import requests



#循环发送请求50次
for i in range(500):
  try:   #捕获异常
     response=requests.get('http://www.baidu.com',timeout=0.1)  #发送请求并且设置超时时间为0.5秒
     print(response.status_code)  #打印状态码
  except ReadError as e1:  #捕获异常,ReadError是超时异常
     print('超时了')  #打印异常信息
  except  HTTPError as e2:     #HTTP异常
        print('HTTP异常')
  except  RequestException as e3:  #请求异常:    
        print('请求错误')
```

# 6.代理服务

在使用网络爬虫经常出现爬一段时间后就无法爬取了,这是因为你的ip被服务器屏蔽了,代理服务器可以为你解决这一麻烦,设置代理时,首先要找到代理地址如122.114.31.177对应的端口号为808，完整的格式为http://150.138.253.72.808

```python


import requests



proxy={'http':'http://http://http://43.138.55.22:8080/','https':'https://http://http://43.138.55.22:8080/'} #设置代理ip及对应的端口号,这里只是一个代理ip
response=requests.get('http://www.baidu.com',proxies=proxy)  #使用代理ip访问百度
print(response.text) #打印百度的源代码
```

# 7.HTML解析BeautifulSoup

BeautifulSoup是一个从HTML和XML文件中提取数据的Python库

导入BeautifulSoup库:from bs4 import BeautifulSoup



```python

from urllib import response
from bs4 import BeautifulSoup
import requests

 
response=requests.get("https://www.baidu.com")   #发送请求
response.encoding='utf-8'   #设置编码字符集
with open ('baidu.html','w',encoding='utf-8') as file:  #打开文件,设置编码字符集,并且以写入模式打开
    file.write(response.text)   #将网页内容写入文件
soup=BeautifulSoup(open('baidu.html',encoding='utf-8'),'lxml')  #使用BeautifulSoup解析文件中额网页代码,设置解析器为lxml
print(soup)  #打印解析后的网页代码
print(soup.prettify) #打印解析后的网页代码,并且格式化输出
```

如果设置解析器为lxml，需要下载pip install lxml



Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种:

- Tag:是标签，标签中有两个重要的属性,例如soup.a.name或者soup.a.attrs就是表示a标签的name属性和attr属性的值

  ​     可以对属性内容进行修改soup.a.name='liuyong'

- NavigableString：

- BeautifulSoup

- Comment

## 重点:BeautifulSoup对象中的find和findall方法

这两个方法是通过BeautifulSoup解析网页的源码通过这两个方法获取源码中自己想要的数据

1. 通过标签获取自己想要的数据所在DOM对象find('ul')

2. 通过文本查找,对自己想要的文本进行匹配find(text=thewangtext)

3. 通过正则表达式,创建一个自己想要的数据的正则表达式,使用re.compile对正则表达式进行匹配,再使text=匹配成功项

4. 通过标签属性来查找find(id='hello')

5. 通过标签属性查找的方式适用大多数标签属性，包括id，style，title，但有 “-”，Class标签属性例外。

   比如html5标签中的data-custom属性，如果我们这样通过定制属性的值进行查找find(attrs={'data-custom':'custom'})一些不常用的属性

6. class是python的保留关键字，所以无法使用class这个关键字。第一种方法：在attrs属性用字典进行传递参数find(attrs={'class':'primaryconsumers'})

```python
  soup=BeautifulSoup(open('douban.html',encoding='utf-8'),'lxml')  #使用BeautifulSoup解析保存到douban.html文件中的网页源码
  movienamereg='[\u4e00-\u9fa5]'  #创建自己要的数据的正则表达式
  moviename=re.compile(movienamereg)    #进行匹配正则表达式
  allmovienames=soup.findAll('span',attrs={'class':'title'},text=moviename)  #获取所有标签为span，class属性为title,并且text值符合正则表达式的标签
  for i in allmovienames:   #遍历所有符合条件的标签
      print(i.text)        #打印标签的text值
  
```



# 8.网络爬虫开发常用的框架Scrapy

1. pip install scrapy
2. 创建一个文件夹用于存放爬虫项目
3. 在文件夹中进入cmd，输入scrapy startproject 项目名  例如scrapy startproject myprojectname

在项目里有以下几个文件夹

1. spiders:用于创建爬虫文件,编写爬虫规则

2. _init_.py：初始化文件

3. items.py:用于数据的定义

4. middlewares.py:定义爬取时的中间件,其中包括SpiderMiddleware(爬虫中间件)，DownloadMiddleware(下载中间件)

5. pipelines.py:用于实现清洗数据验证数据,保存数据

6. setting.py:整个框架的配置文件,主要用于配置爬虫信息如请求头,中间件等

7. scrapy.cfg；项目部署文件,其中定义了项目的配置文件路径等

   

# 9.爬取豆瓣电影top250并保存到数据库中

```python
from cgitb import text
from itertools import count
from tkinter import E
from bs4 import BeautifulSoup
import requests
import re
import pymysql



#访问链接并且讲链接的源码保存到文件中,参数分别是请求链接和请求头
def  geturlinfo(url,UserAgent):
 try:
    response = requests.get(url,headers={'User-Agent': UserAgent})
    response.encoding = 'utf-8'
 except requests.exceptions.RequestException as e:
        print(e)
 finally:
    if(response.status_code == 200):
     with open ('douban.html','w',encoding='utf-8') as file:
         file.write(response.text)


#使用BeautifulSoup解析网页源码获取网页中的电影名字
def getmoviename():
   #创建一个数组来存储电影名字
  movienames=[]
  soup=BeautifulSoup(open('douban.html',encoding='utf-8'),'lxml')  #使用BeautifulSoup解析保存到douban.html文件中的网页源码
  movienamereg= '[\u4e00-\u9fa5]'  #只含有中文的正则表达式
  moviename=re.compile(movienamereg)    #进行匹配正则表达式
  allmovienames=soup.findAll('span',attrs={'class':'title'},text=moviename)  #获取所有标签为span，class属性为title,并且text值符合正则表达式的标签
  for i in allmovienames:   #遍历所有符合条件的标签
    mystr=str(i.text)
    count=mystr.find('/') 
    if(count==-1):       #判断标签中是否含有空格
      #保存到文件中
      #  with open ('moviename.txt','a',encoding='utf-8') as file:
      #     file.write(i.text+'\n')    #返回text值
      movienames.append(i.text)   #将电影名字添加到集合中   
    else:
         continue  #不满足条件的标签跳过当前循环一次
  return movienames
    
     



#连接数据库
def connectdb(moviename,director,starring,country,year,introduce,type):
 conn=pymysql.connect(host='43.138.55.22',user='root',password='123456',db='liu',charset='utf8') #连接数据库
 cursor=conn.cursor()  #创建游标
 info={'moviename':moviename,'director':director,'starring':starring,'country':country,'year':year,'introduce':introduce,'type':type}
 cursor.execute('create table if not exists movie(id int primary key auto_increment,moviename varchar(100),director varchar(100),starring varchar(100),yea varcahr(100),country varcahr(100),type varcahr(100),introduce varchar(100))')  #创建表
 cursor.executemany('insert into movie(moviename,director,starring,country,year,introduce,type) values(%(moviename)s,%(director)s,%(starring)s,%(country)s,%(year)s,%(introduce)s,%(type)s)',info)  #插入数据


#主函数执行程序
start=0
while start<=225:
    url='https://movie.douban.com/top250?start='+str(start)+'&filter='  #定义要访问的链接
    User_Agent='Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Mobile Safari/537.36 Edg/105.0.1343.33'  #定义请求头
    geturlinfo(url,User_Agent)  #调用函数
    for i in getmoviename():
        print(i)
    start+=25

```

